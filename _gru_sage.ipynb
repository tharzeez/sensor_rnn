{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import time"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "from torch.utils.data import TensorDataset, DataLoader"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from tqdm import tqdm_notebook\n", "from sklearn.preprocessing import MinMaxScaler\n", "from numpy import savetxt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define data root directory"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_dir = \"./datasets/\"\n", "# pd.read_csv(data_dir + 'interpolated_df.csv').head() i commented \n", "pd.read_csv(data_dir + 'pm10_amar_2500.csv').head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["label_scalers = {}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_x = []\n", "test_x = {}\n", "test_y = {}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for file in tqdm_notebook(os.listdir(data_dir)): \n", "    # Skipping the files we're not using\n", "    if file[-4:] != \".csv\" or file == \"pjm_hourly_est.csv\":\n", "        continue\n", "    \n", "    print (file, 'file')\n\n", "    # Store csv file in a Pandas DataFrame\n", "    # df = pd.read_csv('{}/{}'.format(data_dir, 'interpolated_df.csv'), parse_dates=[0])\n", "    # df = pd.read_csv('pm10_amar.csv', parse_dates=[0])\n", "    df = pd.read_csv(file, parse_dates=[0])\n", "    # df.head(10)\n", "    # Processing the time data into suitable input formats\n", "    df['hour'] = df.apply(lambda x: x['Datetime'].hour,axis=1)\n", "    df['dayofweek'] = df.apply(lambda x: x['Datetime'].dayofweek,axis=1)\n", "    df['month'] = df.apply(lambda x: x['Datetime'].month,axis=1)\n", "    df['dayofyear'] = df.apply(lambda x: x['Datetime'].dayofyear,axis=1)\n", "    df = df.sort_values(\"Datetime\").drop(\"Datetime\",axis=1)\n", "    print(df.head(10))\n", "    # Scaling the input data\n", "    sc = MinMaxScaler()\n", "    label_sc = MinMaxScaler()\n", "    data = sc.fit_transform(df.values)\n", "    # Obtaining the Scale for the labels(usage data) so that output can be re-scaled to actual value during evaluation\n", "    label_sc.fit(df.iloc[:,0].values.reshape(-1,1))\n", "    label_scalers['amar'] = label_sc\n", "    print(label_sc)\n\n", "    # Define lookback period and split inputs/labels\n", "    lookback = 90\n", "    inputs = np.zeros((len(data)-lookback,lookback,df.shape[1]))\n", "    print(len(data))\n", "    labels = np.zeros(len(data)-lookback)\n", "    for i in range(lookback, len(data)):\n", "        inputs[i-lookback] = data[i-lookback:i]\n", "        labels[i-lookback] = data[i,0]\n", "    inputs = inputs.reshape(-1,lookback,df.shape[1])\n", "    labels = labels.reshape(-1,1)\n\n", "    # Split data into train/test portions and combining all data from different files into a single array\n", "    test_portion = int(0.1*len(inputs))\n", "    if len(train_x) == 0:\n", "        train_x = inputs[:-test_portion]\n", "        train_y = labels[:-test_portion]\n", "    else:\n", "        train_x = np.concatenate((train_x,inputs[:-test_portion]))\n", "        train_y = np.concatenate((train_y,labels[:-test_portion]))\n", "    test_x['amar'] = (inputs[-test_portion:])\n", "    test_y['amar'] = (labels[-test_portion:])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["end of the for loop"]}, {"cell_type": "markdown", "metadata": {}, "source": ["print(test_x)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["batch_size = 1024\n", "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n", "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["is_cuda = torch.cuda.is_available()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if is_cuda:\n", "    device = torch.device(\"cuda\")\n", "else:\n", "    device = torch.device(\"cpu\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class GRUNet(nn.Module):\n", "  def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n", "    super(GRUNet, self).__init__()\n", "    self.hidden_dim = hidden_dim\n", "    self.n_layers = n_layers\n", "    \n", "    self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n", "    self.fc = nn.Linear(hidden_dim, output_dim)\n", "    self.relu = nn.ReLU()\n", "      \n", "  def forward(self, x, h):\n", "    out, h = self.gru(x, h)\n", "    out = self.fc(self.relu(out[:,-1]))\n", "    return out, h\n", "  \n", "  def init_hidden(self, batch_size):\n", "    weight = next(self.parameters()).data\n", "    hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n", "    return hidden"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train(train_loader, learn_rate, hidden_dim=256, EPOCHS=48, model_type=\"GRU\"):\n", "    \n", "    # Setting common hyperparameters\n", "    input_dim = next(iter(train_loader))[0].shape[2]\n", "    output_dim = 1\n", "    n_layers = 2\n", "    # Instantiating the models\n", "    if model_type == \"GRU\":\n", "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n", "    else:\n", "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n", "    model.to(device)\n", "    \n", "    # Defining loss function and optimizer\n", "    criterion = nn.MSELoss()\n", "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n", "    \n", "    model.train()\n", "    print(\"Starting Training of {} model\".format(model_type))\n", "    epoch_times = []\n", "    # Start training loop\n", "    for epoch in range(1,EPOCHS+1):\n", "        start_time = time.clock()\n", "        h = model.init_hidden(batch_size)\n", "        avg_loss = 0.\n", "        counter = 0\n", "        for x, label in train_loader:\n", "            counter += 1\n", "            if model_type == \"GRU\":\n", "                h = h.data\n", "            else:\n", "                h = tuple([e.data for e in h])\n", "            model.zero_grad()\n", "            \n", "            out, h = model(x.to(device).float(), h)\n", "            loss = criterion(out, label.to(device).float())\n", "            loss.backward()\n", "            optimizer.step()\n", "            avg_loss += loss.item()\n", "            if counter%100 == 0:\n", "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n", "        current_time = time.clock()\n", "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n", "        print(\"Total Time Elapsed: {} seconds\".format(str(current_time-start_time)))\n", "        epoch_times.append(current_time-start_time)\n", "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n", "    return model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate(model, test_x, test_y, label_scalers):\n", "    model.eval()\n", "    outputs = []\n", "    targets = []\n", "    start_time = time.clock()\n", "    for i in test_x.keys():\n", "        inp = torch.from_numpy(np.array(test_x[i]))\n", "        labs = torch.from_numpy(np.array(test_y[i]))\n", "        h = model.init_hidden(inp.shape[0])\n", "        out, h = model(inp.to(device).float(), h)\n", "        outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n", "        targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n", "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n", "    sMAPE = 0\n", "    for i in range(len(outputs)):\n", "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n", "    print(\"sMAPE: {}%\".format(sMAPE*100))\n", "    return outputs, targets, sMAPE"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lr = 0.001\n", "gru_model = train(train_loader, lr, model_type=\"GRU\")\n", "# Lstm_model = train(train_loader, lr, model_type=\"LSTM\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gru_outputs, targets, gru_sMAPE = evaluate(gru_model, test_x, test_y, label_scalers)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["savetxt('gru_outputs_epoch16.csv', gru_outputs, delimiter=',')<br>\n", "savetxt('targets_epoch16.csv', targets, delimiter=',')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(14,10))\n", "# plt.subplot(2,2,1)x\n", "plt.plot(gru_outputs[0][-100:], \"-o\", color=\"g\", label=\"Predicted\")\n", "plt.plot(targets[0][-100:], color=\"b\", label=\"Actual\")\n", "plt.ylabel('PM 10')\n", "plt.legend()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}